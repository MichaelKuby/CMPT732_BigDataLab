1.	How did the output change when you submitted with -D mapreduce.job.reduces=3?

With a single reducer, each (key, 1) pair from every line of all input files is aggregated to produce the final output of (key, n), 
where n is the total number of occurrences of that key across all files.

When setting -D mapreduce.job.reduces=3, the number of reducers is increased to three. Consequently, the output data is partitioned 
into three separate output files within the same output directory. Each of these files contains the output of one 
reducer's aggregation operation.

It should be noted that, since the data is partitioned, it's possible that the same key might appear in multiple output files. For 
example, directory A's output file may contain (key, n) and directory B's output file may contain (key, m). If these were to be
combined, the final result would be (key, m+n), which is what we'd expect with a single reducer.

	Why would this be necessary if your job produced large output sets?

If the output sets are big enough, it may be infeasible to reduce efficiently with a single reducer, because the shuffle operation 
requires combining each of the (key, value) pairings together. This may require more memory than is available, or may be considerably 
more inefficient than doing multiple aggregation steps.

By using multiple reducers we can parallelize the reduction step, allowing each of the reducers to work on different sets of keys at 
the same time. The tradeoff is that we may need to include another aggregation step.

This also allows for scalability. If the amount of data grows, we're able to scale as necessary.

2. 	How was the -D mapreduce.job.reduces=0 output different?

Setting the number of reducers to zero essentially skips the reduce phase, and the output is directly written from the mappers to the 
HDFS.

The output when using -D mapreduce.job.reduces=0 consisted of a single directory containing three separate files. Each file held the 
output directly from the map phase, mapping input lines of text to (key, 1). In this case, each key represents a parsed token from each 
line of text. Notably, the keys in each file are not unique since no combining or reducing has been undertaken.

Had a combiner been used, each file would contain aggregated key-value pairs in the form of (key, n), where n represents the number of 
times a particular key appeared in the data chunk processed by that specific mapper. Note that even with a combiner, the 
keys wouldn't be unique across the entire dataset; they would only be locally aggregated per mapper's output.
