1. What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

By viewing the files in the filesystem we can get a clear picture of what's going on with the input:

Found 8 items
-rw-r--r--   2 ggbaker supergroup  272333102 2020-12-16 09:13 /courses/732/wordcount-5/7.txt.gz
-rw-r--r--   2 ggbaker supergroup      93397 2020-12-16 09:03 /courses/732/wordcount-5/F.txt.gz
-rw-r--r--   2 ggbaker supergroup      84223 2020-12-16 09:58 /courses/732/wordcount-5/S.txt.gz
-rw-r--r--   2 ggbaker supergroup   44054520 2020-12-16 09:57 /courses/732/wordcount-5/d.txt.gz
-rw-r--r--   2 ggbaker supergroup   93890605 2020-12-16 09:32 /courses/732/wordcount-5/g.txt.gz
-rw-r--r--   2 ggbaker supergroup  116015482 2020-12-16 09:41 /courses/732/wordcount-5/m.txt.gz
-rw-r--r--   2 ggbaker supergroup   19394837 2020-12-16 09:30 /courses/732/wordcount-5/o.txt.gz
-rw-r--r--   2 ggbaker supergroup   79300825 2020-12-16 10:00 /courses/732/wordcount-5/s.txt.gz

If we look at the fourth column of integers, which denotes the size of the inputs, we can see that the size of the files vary significantly. So when the original partitions are created, by default we will get unequally distributed partitions based on the original file 
structure. While the program runs, when a node completes a task on one of the smaller partitions, it then sits idle, while an executor node working on a larger partition continues to work away.

By changing the number of partitions to some number a fair bit larger than 8 (I used 8 * 10 = 80) we get 80, presumably equally distributed partitions to be worked on. Now, when one executor node completes a task on one of the partitions, there will be other partitions queued 
up to be worked on. If any executor nodes are ever sitting idle, it should be near the very end of the job while other executor nodes are working on the very last (and now much smaller) partitions.

2. The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

The big difference between wordcount-5 and wordcount-3 is in that the there are 101 files in the wordcount-3 directory and, notably, all but one of those files are within one order of magnitude in size. In other words, they are already relatively well partitioned for 
computation. To answer the question for why it might actually be slower, we note that .repartition() is a shuffle operation, requiring an all-to-all migration of information over the network. The overhead required to serialize the data and shuffle it over the network into new 
partitions is a large cost considering the original partitioning schema is already sufficient.

3. How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

We might think about modifying the inputs themselves by doing some manual file splitting and repartitioning of the files (using say zsh 
or bash) before loading them in to Spark. This would have the effect of removing the repartitioning step every time the code is run, 
removing a significant amount of run-time overhead.

4. When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the 
desktop/laptop where you were testing)?

Note first that my computer has 4 cores. The takeaway was that, using .numPartitions(n), if n >= the number of cores available, there was basically no difference in the run time, until n became very large. Why? If n < the number of cores, we were failing to make use of all the 
cores available, thus the degree of parallelism was suboptimal. Once n >= the number of cores, we were making ideal use of the cores available to us, and since Spark evenly distributes the data among the partitions, any number above 4 and below an extremely high number of 
partitions gave us reasonable speed. Numbers too high accrued speed loss due to overhead. It also seems reasonable to choose n such that n evenly divides the number of cores; by doing so we hope that even the final partitions are making full use of the available cores.

Here are a few samples of output from my times:

1 partition: 	spark-submit euler.py 5000000  9.33s user 0.84s system 9% cpu 1:49.83 total 
2 partitions: 	spark-submit euler.py 5000000  9.07s user 0.82s system 16% cpu 59.776 total
4 partitions:	spark-submit euler.py 5000000  10.90s user 0.98s system 28% cpu 42.000 total
16 partitions:	spark-submit euler.py 5000000  9.81s user 1.13s system 26% cpu 41.543 total
60 partitions:	spark-submit euler.py 5000000  12.35s user 0.99s system 32% cpu 40.485 total
120 partitions: spark-submit euler.py 5000000  14.05s user 1.08s system 37% cpu 40.449 total
240 partitions: spark-submit euler.py 5000000  14.71s user 1.17s system 35% cpu 44.245 total

Note that once we were using 4 cores, most times seemed reasonable. At greater than 240 we suspect overhead slowdown.

5. How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation?
